{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bb15a7",
   "metadata": {},
   "source": [
    "### What is activation functions?\n",
    "Definition of activation function:- Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9a482",
   "metadata": {},
   "source": [
    "![](activationfunction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd5f87d",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dadc08",
   "metadata": {},
   "source": [
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efe7e2",
   "metadata": {},
   "source": [
    "![](sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5318a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9480bc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597dc170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296672a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.780892883885469e-25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7cd1128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.549833997312478"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b13335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b496f2f",
   "metadata": {},
   "source": [
    "## Tanh function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7988533",
   "metadata": {},
   "source": [
    "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dc8f88",
   "metadata": {},
   "source": [
    "![](tanh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e69e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return ((math.exp(x)-math.exp(-x))/(math.exp(x)+math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1a1f612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(-56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f47811f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80af907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4621171572600098"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3f5a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8bc3",
   "metadata": {},
   "source": [
    "## ReLU Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100ba5e",
   "metadata": {},
   "source": [
    "In the context of artificial neural networks, the rectifier or ReLU activation function is an activation function defined as the positive part of its argument: {\\displaystyle f(x)=x^{+}=\\max} where x is the input to a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ca7d1",
   "metadata": {},
   "source": [
    "![](relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29ff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e6c2191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fdfe2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a196f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e186b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c290a35e",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da973b",
   "metadata": {},
   "source": [
    "The Leaky ReLU (LReLU or LReL) modifies the function to allow small negative values when the input is less than zero. The leaky rectifier allows for a small, non-zero gradient when the unit is saturated and not active."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea408c",
   "metadata": {},
   "source": [
    "![](leakyrelu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a13db084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyrelu(x):\n",
    "    return max(0.1*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b5bd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leakyrelu(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72feceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leakyrelu(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e6ba15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leakyrelu(-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e88de785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leakyrelu(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fb5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3e541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
